_target_: src.models.classification_module.ClassificationModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10




# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: mnist.yaml
  - override /model: vit.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["mnist", "vit"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 200
  gradient_clip_val: 0.5
  accelerator: gpu
  devices: [0, 1, 2, 3, 4, 5, 6, 7]

model:
  net:
    _target_: src.models.components.vit.ViT
    input_channel: 1
    img_size: 28
    patch_size: 7
    dim: 128
    num_of_heads: 4
    dropout: 0.3
    expansion: 2
    depth: 6
    output_size: 10


logger:
  wandb:
    project: "MNIST-ViT"
    tags: ['ViT']
    name: "ViT"


